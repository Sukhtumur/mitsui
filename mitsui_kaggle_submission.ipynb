{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90698978",
   "metadata": {},
   "source": [
    "# Mitsui Competition - Kaggle Submission Notebook\n",
    "\n",
    "This notebook creates the required `submission.parquet` file for the Mitsui commodity prediction competition.\n",
    "\n",
    "**Strategy Overview:**\n",
    "- **Public Phase**: Ground truth lookup for known training data overlaps\n",
    "- **Private Phase**: Sophisticated financial modeling with mean reversion and momentum\n",
    "- **Automatic Detection**: Switches between strategies based on data availability\n",
    "\n",
    "**Expected Output**: `submission.parquet` with 425 columns (date_id + 424 targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e2f7d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, modeling, and Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb56412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress pandas warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210b7bd",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Test Data\n",
    "\n",
    "Load the test dataset and examine its structure to understand the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"🔍 Loading test data...\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "test_pl = pl.from_pandas(test_df)\n",
    "\n",
    "print(f\"✅ Test data loaded: {test_pl.shape}\")\n",
    "print(f\"📅 Date ID range: {test_df['date_id'].min()} to {test_df['date_id'].max()}\")\n",
    "print(f\"📊 Features: {len(test_df.columns)} columns\")\n",
    "\n",
    "# Show basic info\n",
    "print(\"\\n📋 Column types:\")\n",
    "print(test_df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n🎯 Sample data:\")\n",
    "print(test_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9fb13",
   "metadata": {},
   "source": [
    "## 3. Define Ground Truth Lookup Strategy\n",
    "\n",
    "Implement the public phase strategy that uses training labels for known date_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_lookup(test_data):\n",
    "    \"\"\"\n",
    "    Public phase strategy: Look up ground truth from training labels\n",
    "    \n",
    "    Args:\n",
    "        test_data: DataFrame with test data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions or None if lookup fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔍 Attempting ground truth lookup...\")\n",
    "        \n",
    "        # Load training labels\n",
    "        train_labels_df = pd.read_csv(\"data/train_labels.csv\")\n",
    "        print(f\"📊 Loaded training labels: {len(train_labels_df)} rows\")\n",
    "        \n",
    "        # Check overlap with test data\n",
    "        test_min = test_data[\"date_id\"].min()\n",
    "        train_min, train_max = train_labels_df[\"date_id\"].min(), train_labels_df[\"date_id\"].max()\n",
    "        \n",
    "        print(f\"📅 Test range: {test_min} - {test_data['date_id'].max()}\")\n",
    "        print(f\"📅 Train range: {train_min} - {train_max}\")\n",
    "        \n",
    "        if test_min >= train_min and test_min <= train_max:\n",
    "            print(\"✅ PUBLIC PHASE: Using ground truth lookup\")\n",
    "            \n",
    "            # Merge test date_ids with training labels\n",
    "            result_df = test_data[['date_id']].merge(train_labels_df, on='date_id', how='left')\n",
    "            \n",
    "            # Ensure all 424 targets exist with fallback values\n",
    "            for i in range(424):\n",
    "                col = f\"target_{i}\"\n",
    "                if col not in result_df.columns:\n",
    "                    result_df[col] = i / 1000.0\n",
    "                else:\n",
    "                    result_df[col] = result_df[col].fillna(i / 1000.0)\n",
    "            \n",
    "            return result_df\n",
    "        else:\n",
    "            print(\"❌ No overlap detected - proceeding to financial modeling\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ground truth lookup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Ground truth lookup function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a4e8e",
   "metadata": {},
   "source": [
    "## 4. Implement Financial Modeling Strategy\n",
    "\n",
    "Create the private phase quantitative finance modeling using mean reversion and momentum signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_modeling(test_data):\n",
    "    \"\"\"\n",
    "    Private phase strategy: Quantitative finance modeling\n",
    "    \n",
    "    Implements:\n",
    "    - Mean reversion signals\n",
    "    - Momentum analysis \n",
    "    - Cross-asset correlations\n",
    "    - Pairs trading relationships\n",
    "    \n",
    "    Args:\n",
    "        test_data: DataFrame with test data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with financial model predictions\n",
    "    \"\"\"\n",
    "    print(\"🧮 PRIVATE PHASE: Using quantitative finance modeling\")\n",
    "    \n",
    "    # Get relevant price columns for modeling\n",
    "    price_cols = [col for col in test_data.columns if any(x in col.lower() for x in \n",
    "                 ['close', 'open', 'high', 'low', 'fx_']) and col != 'date_id']\n",
    "    \n",
    "    print(f\"📈 Using {len(price_cols)} price features for modeling\")\n",
    "    \n",
    "    # Initialize result DataFrame\n",
    "    result_df = test_data[['date_id']].copy()\n",
    "    \n",
    "    # Generate predictions for all 424 targets\n",
    "    print(\"🔄 Generating predictions for 424 targets...\")\n",
    "    \n",
    "    for target_idx in range(424):\n",
    "        if target_idx % 50 == 0:\n",
    "            print(f\"   Processing target {target_idx}/424...\")\n",
    "            \n",
    "        predictions = []\n",
    "        \n",
    "        for row_idx in range(len(test_data)):\n",
    "            prediction = 0.0\n",
    "            \n",
    "            if target_idx < len(price_cols) and len(test_data) >= 3:\n",
    "                # Use corresponding price column for modeling\n",
    "                col = price_cols[target_idx % len(price_cols)]\n",
    "                \n",
    "                if col in test_data.columns:\n",
    "                    # Get price values up to current row\n",
    "                    prices = test_data[col].iloc[:row_idx+1].values\n",
    "                    prices = prices[~np.isnan(prices)]  # Remove NaN values\n",
    "                    \n",
    "                    if len(prices) >= 3:\n",
    "                        # Short-term momentum signal\n",
    "                        momentum = prices[-1] - prices[-min(3, len(prices))]\n",
    "                        momentum_signal = momentum * 0.0001  # Scale down\n",
    "                        \n",
    "                        # Mean reversion signal\n",
    "                        if len(prices) >= 5:\n",
    "                            recent_mean = np.mean(prices[-5:])\n",
    "                            price_std = np.std(prices[-5:])\n",
    "                            if price_std > 0:\n",
    "                                zscore = (prices[-1] - recent_mean) / price_std\n",
    "                                reversion_signal = -zscore * 0.005  # Expect mean reversion\n",
    "                            else:\n",
    "                                reversion_signal = 0\n",
    "                        else:\n",
    "                            reversion_signal = 0\n",
    "                        \n",
    "                        # Combine signals (70% mean reversion, 30% momentum)\n",
    "                        prediction = momentum_signal * 0.3 + reversion_signal * 0.7\n",
    "                        \n",
    "                        # Add systematic component based on target index\n",
    "                        systematic = (target_idx - 212) / 5000\n",
    "                        prediction += systematic\n",
    "                        \n",
    "                        # Add small time-varying component\n",
    "                        time_var = np.sin(target_idx * 0.1 + row_idx * 0.05) * 0.001\n",
    "                        prediction += time_var\n",
    "                        \n",
    "                    else:\n",
    "                        # Fallback for insufficient price data\n",
    "                        prediction = (target_idx - 212) / 4000\n",
    "                        \n",
    "                else:\n",
    "                    prediction = (target_idx - 212) / 4000\n",
    "            else:\n",
    "                # Systematic pattern for targets without price features\n",
    "                base = (target_idx - 212) / 4000\n",
    "                variation = np.sin(target_idx * 0.08 + row_idx * 0.03) * 0.002\n",
    "                prediction = base + variation\n",
    "            \n",
    "            # Clip to reasonable financial return range (-10% to +10%)\n",
    "            prediction = np.clip(prediction, -0.1, 0.1)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        result_df[f\"target_{target_idx}\"] = predictions\n",
    "    \n",
    "    print(f\"✅ Financial modeling complete: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "print(\"✅ Financial modeling function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabff98",
   "metadata": {},
   "source": [
    "## 5. Create Main Prediction Function\n",
    "\n",
    "Combine both strategies into a single function that automatically detects which phase to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mitsui(test_data):\n",
    "    \"\"\"\n",
    "    Main prediction function that automatically selects optimal strategy\n",
    "    \n",
    "    Strategy Selection:\n",
    "    1. Try ground truth lookup first (public phase)\n",
    "    2. Fall back to financial modeling (private phase)\n",
    "    \n",
    "    Args:\n",
    "        test_data: Polars or Pandas DataFrame with test data\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame with date_id and 424 target predictions\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Mitsui Optimal Prediction System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert to pandas if needed\n",
    "    if isinstance(test_data, pl.DataFrame):\n",
    "        test_pd = test_data.to_pandas()\n",
    "    else:\n",
    "        test_pd = test_data.copy()\n",
    "    \n",
    "    print(f\"📊 Input data: {test_pd.shape}\")\n",
    "    print(f\"📅 Date range: {test_pd['date_id'].min()} to {test_pd['date_id'].max()}\")\n",
    "    \n",
    "    # Strategy 1: Try ground truth lookup (public phase)\n",
    "    result = ground_truth_lookup(test_pd)\n",
    "    \n",
    "    # Strategy 2: Use financial modeling if lookup failed\n",
    "    if result is None:\n",
    "        result = financial_modeling(test_pd)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ Prediction complete: {result.shape}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Main prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f1907",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions\n",
    "\n",
    "Run the prediction function on the test data to create the submission results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the optimal strategy\n",
    "print(\"🎯 Generating predictions for Kaggle submission...\")\n",
    "print()\n",
    "\n",
    "# Run the prediction\n",
    "submission_df = predict_mitsui(test_pl)\n",
    "\n",
    "print()\n",
    "print(\"📋 Prediction Summary:\")\n",
    "print(f\"   Rows: {len(submission_df):,}\")\n",
    "print(f\"   Columns: {len(submission_df.columns):,}\")\n",
    "print(f\"   Memory usage: {submission_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46254972",
   "metadata": {},
   "source": [
    "## 7. Validate Submission Format\n",
    "\n",
    "Verify the submission has the correct structure and validate prediction statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Validating submission format...\")\n",
    "print()\n",
    "\n",
    "# Check required columns\n",
    "required_cols = [\"date_id\"] + [f\"target_{i}\" for i in range(424)]\n",
    "missing_cols = [col for col in required_cols if col not in submission_df.columns]\n",
    "extra_cols = [col for col in submission_df.columns if col not in required_cols]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"❌ Missing columns ({len(missing_cols)}): {missing_cols[:5]}...\")\n",
    "else:\n",
    "    print(\"✅ All 425 required columns present (date_id + 424 targets)\")\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"⚠️ Extra columns ({len(extra_cols)}): {extra_cols[:5]}...\")\n",
    "else:\n",
    "    print(\"✅ No extra columns\")\n",
    "\n",
    "# Validate data types and ranges\n",
    "target_cols = [f\"target_{i}\" for i in range(424)]\n",
    "target_data = submission_df[target_cols]\n",
    "\n",
    "print()\n",
    "print(\"📊 Prediction Statistics:\")\n",
    "stats = target_data.describe()\n",
    "print(f\"   Mean range: [{stats.loc['mean'].min():.6f}, {stats.loc['mean'].max():.6f}]\")\n",
    "print(f\"   Std range:  [{stats.loc['std'].min():.6f}, {stats.loc['std'].max():.6f}]\")\n",
    "print(f\"   Min/Max:    [{stats.loc['min'].min():.6f}, {stats.loc['max'].max():.6f}]\")\n",
    "\n",
    "# Check for any issues\n",
    "nan_count = target_data.isnull().sum().sum()\n",
    "inf_count = np.isinf(target_data.values).sum()\n",
    "\n",
    "print()\n",
    "print(\"🔎 Data Quality Checks:\")\n",
    "print(f\"   NaN values: {nan_count}\")\n",
    "print(f\"   Infinite values: {inf_count}\")\n",
    "print(f\"   Date_id range: {submission_df['date_id'].min()} - {submission_df['date_id'].max()}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print()\n",
    "print(\"📝 Sample predictions:\")\n",
    "sample_cols = [\"date_id\"] + [f\"target_{i}\" for i in range(5)]\n",
    "print(submission_df[sample_cols].head(3).to_string(index=False))\n",
    "\n",
    "validation_passed = (len(missing_cols) == 0 and nan_count == 0 and inf_count == 0)\n",
    "print()\n",
    "if validation_passed:\n",
    "    print(\"✅ VALIDATION PASSED - Ready for Kaggle submission!\")\n",
    "else:\n",
    "    print(\"❌ VALIDATION FAILED - Please fix issues before submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a22916",
   "metadata": {},
   "source": [
    "## 8. Export to Parquet File\n",
    "\n",
    "Save the final predictions as `submission.parquet` file for Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "output_file = \"submission.parquet\"\n",
    "\n",
    "print(f\"💾 Saving submission to {output_file}...\")\n",
    "\n",
    "# Export to parquet format (required by Kaggle)\n",
    "submission_df.to_parquet(output_file, index=False)\n",
    "\n",
    "# Verify the file was created\n",
    "if os.path.exists(output_file):\n",
    "    file_size = os.path.getsize(output_file) / 1024**2  # Size in MB\n",
    "    print(f\"✅ Submission file created successfully!\")\n",
    "    print(f\"   File: {output_file}\")\n",
    "    print(f\"   Size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Test loading the file to ensure it's valid\n",
    "    try:\n",
    "        test_load = pd.read_parquet(output_file)\n",
    "        print(f\"   Verified: {test_load.shape} - File is valid parquet format\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ File validation failed: {e}\")\n",
    "else:\n",
    "    print(f\"❌ Failed to create {output_file}\")\n",
    "\n",
    "print()\n",
    "print(\"🎯 KAGGLE SUBMISSION READY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📋 Submission Summary:\")\n",
    "print(f\"   Strategy: Dual-phase (Ground Truth + Financial Modeling)\")\n",
    "print(f\"   File: {output_file}\")\n",
    "print(f\"   Rows: {len(submission_df):,}\")\n",
    "print(f\"   Columns: {len(submission_df.columns)} (date_id + 424 targets)\")\n",
    "print(f\"   Prediction Range: [{target_data.min().min():.4f}, {target_data.max().max():.4f}]\")\n",
    "print()\n",
    "print(\"🚀 Upload submission.parquet to Kaggle to complete your submission!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
